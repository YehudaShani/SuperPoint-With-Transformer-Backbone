data:
    name: 'synthetic'
    primitives: 'all'
    truncate: {draw_ellipses: 0.3, draw_stripes: 0.2, gaussian_noise: 0.1}
    validation_size: 500
    data_dir: './data/synthetic_shapes'#where to save generated data
    preprocessing:
        blur_size: 21
        resize: [120, 160]
    augmentation:
        photometric:
            enable: true
            primitives: [
                'random_brightness', 'random_contrast', 'additive_speckle_noise',
                'additive_gaussian_noise', 'additive_shade', 'motion_blur' ]
            params:
                random_brightness: {max_abs_change: 75}
                random_contrast: {strength_range: [0.3, 1.8]}
                additive_gaussian_noise: {stddev_range: [0, 15]}
                additive_speckle_noise: {prob_range: [0, 0.0035]}
                additive_shade:
                    transparency_range: [-0.5, 0.8]
                    kernel_size_range: [50, 100]
                    nb_ellipses: 20
                motion_blur: {max_kernel_size: 7}
        homographic:
            enable: true
            params:
                translation: true
                rotation: true
                scaling: true
                perspective: true
                scaling_amplitude: 0.2
                perspective_amplitude_x: 0.2
                perspective_amplitude_y: 0.2
                patch_ratio: 0.8
                max_angle: 1.57
                allow_artifacts: true
                translation_overflow: 0.05
            valid_border_margin: 2
model:
  name: 'magicpoint_swin'
  using_bn: true
  pretrained_model: 'none'
  grid_size: 8

  backbone:
    backbone_type: 'Swin'
    swin:
      name: 'swin_tiny_patch4_window7_224'
      pretrained: true
      normalize: true
      img_size: [ 120, 160 ]   # <-- added
      freeze_backbone: true   # <— add this



  det_head:
    # If you keep 128, the class adds a 1x1 neck to map 192->128.
    # Or set feat_in_dim: 192 to skip the neck.
    feat_in_dim: 192

  det_thresh: 0.015
  nms: 0
  topk: -1

solver:
  grid_size: 8
  epoch: 20
  base_lr: 0.001
  train_batch_size: 64
  test_batch_size: 64
  save_dir: './export'
  model_name: 'mg_syn_swin'

         # try 128–256 (watch VRAM)
  eval_max_batches: 5        # cap eval per epoch so training starts fast
  skip_initial_eval: true    # jump straight into training
  log_every: 100             # tqdm print frequency
  weight_decay: 0.0

